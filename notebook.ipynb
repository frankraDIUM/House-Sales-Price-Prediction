{"cells":[{"source":"import pandas as pd\nimport numpy as np\n\n# 1. Load data\ndf = pd.read_csv(\"house_sales.csv\")\n\n# Calculate the number of missing values in the 'city' column and save to missing_city\nmissing_city = (df['city'].isna() | (df['city'] == '--')).sum()\n\n# Output the result\nprint(missing_city)\n\n\n# 2. Clean data\ndf['house_id'] = df['house_id'].astype('category')\n\n# City: Replace missing/invalid with 'Unknown'\ndf['city'] = df['city'].str.replace('--', 'Unknown')\ndf['city'] = df['city'].astype('category')\n\n# Sale Price: Remove missing entries\ndf = df.dropna(subset=['sale_price'])\ndf['sale_price'] = df['sale_price'].astype('int')\n\n# Sale Date: Replace missing with 2023-01-01\ndf['sale_date'] = df['sale_date'].fillna('2023-01-01')\n\n# Months Listed: Replace missing with mean, rounded to 1 decimal place\nmean_months = round(df['months_listed'].mean(), 1)\ndf['months_listed'] = df['months_listed'].fillna(mean_months)\ndf['months_listed'] = df['months_listed'].astype('float')\n\n# Bedrooms: Replace missing with mean, rounded to nearest integer\nmean_bedrooms = round(df['bedrooms'].mean())\ndf['bedrooms'] = df['bedrooms'].fillna(mean_bedrooms).astype(int)\n\n# House Type: Clean variations and replace missing with most common\nthe_map = {'Det.': 'Detached', 'Semi':'Semi-detached', 'Terr.':'Terraced' }\ndf['house_type'] = df['house_type'].replace(the_map)\n\n# Filter for only allowed types or replace missing/invalid with mode\nmost_common_type = df['house_type'].mode()[0]\nallowed_types = [\"Terraced\", \"Semi-detached\", \"Detached\"]\ndf.loc[~df['house_type'].isin(allowed_types), 'house_type'] = most_common_type\ndf['house_type'] = df['house_type'].fillna(most_common_type)\n\n# Area: Remove ' sq.m.' string, convert to float, fill missing with mean\nif df['area'].dtype == object:\n    df['area'] = df['area'].str.replace(' sq.m.', '', regex=False).astype(float)\ndf['area'] = df['area'].round(1)\nmean_area = round(df['area'].mean(), 1)\ndf['area'] = df['area'].fillna(mean_area)\n\nclean_data = df\n\n# Preview cleaned data\nprint(clean_data.head(5))\n\n\n# 3. Compute Sale price by number of bedrooms\n# Group by bedrooms and calculate average price and variance\nhome = df = pd.read_csv('house_sales.csv')\nprice_by_rooms = home.groupby('bedrooms').agg(avg_price=('sale_price','mean'), var_price = ('sale_price','var')).round(1).reset_index(drop=False)\nprint(price_by_rooms)\n\n\n# 4. Fit a baseline model to predict the sale price of a house.\n# Load training and validation data\ntraining_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('validation.csv')\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as mse\n\n# Preprocessing: Drop non-predictive columns\n# house_id and sale_date are identifiers/metadata, not features\ntraining_data = training_data.drop(['sale_date', 'house_id'], axis=1)\ntest_data = test_data.drop(['sale_date'], axis=1)\n\n# Encoding: Convert categorical variables into dummy/indicator variables\ntraining_data = pd.get_dummies(training_data, drop_first=True, dtype=int)\ntest_data = pd.get_dummies(test_data, drop_first=True, dtype=int)\n\n# Feature Selection: Separate features (X) from the target (y)\nX = training_data.drop(['sale_price'], axis=1).values\ny = training_data['sale_price'].values\n\n# Data Splitting: Create training and internal testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n\n# Feature Scaling: Normalize data so all features are on the same scale\nscale = StandardScaler()\n\n# Fit only on training data to prevent data leakage\ntrans_train_data = scale.fit_transform(X_train)\ntrans_test_data = scale.transform(X_test)\n\n# Model Training: Initialize and fit the Linear Regression model\nlinreg = LinearRegression()\n\n# Evaluation: Predict on the internal test set and calculate RMSE\nlinreg.fit(trans_train_data, y_train)\nlr_ypred = linreg.predict(trans_test_data)\nrmse = np.sqrt(mse(y_test, lr_ypred))\n\n# Final Prediction: Prepare the validation data for submission\nref = test_data.drop(['house_id'], axis=1).values\ntest_data_mod = scale.fit_transform(ref)\n\n# Results: Map predictions back to the original House IDs\nbase_result = test_data[['house_id']].assign(price = linreg.predict(test_data_mod))\n\n# Display final results\nprint(f\"Model RMSE: {rmse}\")\nprint(base_result)\n\n\n\n# 5. Fit a comparison model to predict the sale price of a house.\n# Model Initialization\n# Initialize the Random Forest Regressor with a fixed random state for reproducibility\nrfr = RandomForestRegressor(random_state = 9)\n\n# Hyperparameter Grid Definition\n# Define the search space for the model to find the best combination of settings\nparam_rfs = {\n    'max_depth':[6, 7, 8],\n    'n_estimators': [100, 200],\n    'min_samples_leaf':[5, 7, 10],\n    'min_samples_split': [2, 4, 6, 8]\n}\n\n\n\n# Grid Search Configuration\n# Set up GridSearchCV to cross-validate different combinations\n# scoring='neg_root_mean_squared_error' uses RMSE (negative because higher is better in sklearn)\n# n_jobs=-1 uses all available CPU cores for faster training\ngrid_rfr = GridSearchCV(estimator=rfr, param_grid=param_rfs, scoring='neg_root_mean_squared_error', n_jobs=-1)\n\n\n# Training\n# Run the grid search on the scaled training data\ngrid_rfr.fit(trans_train_data, y_train)\n\n# Model Selection\n# Extract the best-performing model found during the search\nbest_rfr = grid_rfr.best_estimator_\n\n# Evaluation\n# Use the optimized model to predict on the internal test set\nrfr_y_pred = best_rfr.predict(trans_test_data)\n\n# Calculate the Root Mean Squared Error for the Random Forest model\nrfr_rmse = np.sqrt(mse(y_test, rfr_y_pred))\n\n# Final Output Generation\n# Predict prices for the validation set using the best Random Forest model\n# Note: Ensure test_data_mod was scaled using scale.transform() from earlier\ncompare_result = test_data[['house_id']].assign(price = best_rfr.predict(test_data_mod))\n\nprint(compare_result.head(10))\n\n# Display the performance metric\nprint(f\"Random Forest Best RMSE: {rfr_rmse}\")\nprint(f\"Best Parameters: {grid_rfr.best_params_}\")","metadata":{"executionCancelledAt":null,"executionTime":45224,"lastExecutedAt":1766446410274,"lastExecutedByKernel":"38f3fd18-861f-43d0-a77e-e2d2fc27266a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np\n\n# 1. Load data\ndf = pd.read_csv(\"house_sales.csv\")\n\n# Calculate the number of missing values in the 'city' column and save to missing_city\nmissing_city = (df['city'].isna() | (df['city'] == '--')).sum()\n\n# Output the result\nprint(missing_city)\n\n\n# 2. Clean data\ndf['house_id'] = df['house_id'].astype('category')\n\n# City: Replace missing/invalid with 'Unknown'\ndf['city'] = df['city'].str.replace('--', 'Unknown')\ndf['city'] = df['city'].astype('category')\n\n# Sale Price: Remove missing entries\ndf = df.dropna(subset=['sale_price'])\ndf['sale_price'] = df['sale_price'].astype('int')\n\n# Sale Date: Replace missing with 2023-01-01\ndf['sale_date'] = df['sale_date'].fillna('2023-01-01')\n\n# Months Listed: Replace missing with mean, rounded to 1 decimal place\nmean_months = round(df['months_listed'].mean(), 1)\ndf['months_listed'] = df['months_listed'].fillna(mean_months)\ndf['months_listed'] = df['months_listed'].astype('float')\n\n# Bedrooms: Replace missing with mean, rounded to nearest integer\nmean_bedrooms = round(df['bedrooms'].mean())\ndf['bedrooms'] = df['bedrooms'].fillna(mean_bedrooms).astype(int)\n\n# House Type: Clean variations and replace missing with most common\nthe_map = {'Det.': 'Detached', 'Semi':'Semi-detached', 'Terr.':'Terraced' }\ndf['house_type'] = df['house_type'].replace(the_map)\n\n# Filter for only allowed types or replace missing/invalid with mode\nmost_common_type = df['house_type'].mode()[0]\nallowed_types = [\"Terraced\", \"Semi-detached\", \"Detached\"]\ndf.loc[~df['house_type'].isin(allowed_types), 'house_type'] = most_common_type\ndf['house_type'] = df['house_type'].fillna(most_common_type)\n\n# Area: Remove ' sq.m.' string, convert to float, fill missing with mean\nif df['area'].dtype == object:\n    df['area'] = df['area'].str.replace(' sq.m.', '', regex=False).astype(float)\ndf['area'] = df['area'].round(1)\nmean_area = round(df['area'].mean(), 1)\ndf['area'] = df['area'].fillna(mean_area)\n\nclean_data = df\n\n# Preview cleaned data\nprint(clean_data.head(5))\n\n\n# 3. Compute Sale price by number of bedrooms\n# Group by bedrooms and calculate average price and variance\nhome = df = pd.read_csv('house_sales.csv')\nprice_by_rooms = home.groupby('bedrooms').agg(avg_price=('sale_price','mean'), var_price = ('sale_price','var')).round(1).reset_index(drop=False)\nprint(price_by_rooms)\n\n\n# 4. Fit a baseline model to predict the sale price of a house.\n# Load training and validation data\ntraining_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('validation.csv')\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as mse\n\n# Preprocessing: Drop non-predictive columns\n# house_id and sale_date are identifiers/metadata, not features\ntraining_data = training_data.drop(['sale_date', 'house_id'], axis=1)\ntest_data = test_data.drop(['sale_date'], axis=1)\n\n# Encoding: Convert categorical variables into dummy/indicator variables\ntraining_data = pd.get_dummies(training_data, drop_first=True, dtype=int)\ntest_data = pd.get_dummies(test_data, drop_first=True, dtype=int)\n\n# Feature Selection: Separate features (X) from the target (y)\nX = training_data.drop(['sale_price'], axis=1).values\ny = training_data['sale_price'].values\n\n# Data Splitting: Create training and internal testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n\n# Feature Scaling: Normalize data so all features are on the same scale\nscale = StandardScaler()\n\n# Fit only on training data to prevent data leakage\ntrans_train_data = scale.fit_transform(X_train)\ntrans_test_data = scale.transform(X_test)\n\n# Model Training: Initialize and fit the Linear Regression model\nlinreg = LinearRegression()\n\n# Evaluation: Predict on the internal test set and calculate RMSE\nlinreg.fit(trans_train_data, y_train)\nlr_ypred = linreg.predict(trans_test_data)\nrmse = np.sqrt(mse(y_test, lr_ypred))\n\n# Final Prediction: Prepare the validation data for submission\nref = test_data.drop(['house_id'], axis=1).values\ntest_data_mod = scale.fit_transform(ref)\n\n# Results: Map predictions back to the original House IDs\nbase_result = test_data[['house_id']].assign(price = linreg.predict(test_data_mod))\n\n# Display final results\nprint(f\"Model RMSE: {rmse}\")\nprint(base_result)\n\n\n\n# 5. Fit a comparison model to predict the sale price of a house.\n# Model Initialization\n# Initialize the Random Forest Regressor with a fixed random state for reproducibility\nrfr = RandomForestRegressor(random_state = 9)\n\n# Hyperparameter Grid Definition\n# Define the search space for the model to find the best combination of settings\nparam_rfs = {\n    'max_depth':[6, 7, 8],\n    'n_estimators': [100, 200],\n    'min_samples_leaf':[5, 7, 10],\n    'min_samples_split': [2, 4, 6, 8]\n}\n\n\n\n# Grid Search Configuration\n# Set up GridSearchCV to cross-validate different combinations\n# scoring='neg_root_mean_squared_error' uses RMSE (negative because higher is better in sklearn)\n# n_jobs=-1 uses all available CPU cores for faster training\ngrid_rfr = GridSearchCV(estimator=rfr, param_grid=param_rfs, scoring='neg_root_mean_squared_error', n_jobs=-1)\n\n\n# Training\n# Run the grid search on the scaled training data\ngrid_rfr.fit(trans_train_data, y_train)\n\n# Model Selection\n# Extract the best-performing model found during the search\nbest_rfr = grid_rfr.best_estimator_\n\n# Evaluation\n# Use the optimized model to predict on the internal test set\nrfr_y_pred = best_rfr.predict(trans_test_data)\n\n# Calculate the Root Mean Squared Error for the Random Forest model\nrfr_rmse = np.sqrt(mse(y_test, rfr_y_pred))\n\n# Final Output Generation\n# Predict prices for the validation set using the best Random Forest model\n# Note: Ensure test_data_mod was scaled using scale.transform() from earlier\ncompare_result = test_data[['house_id']].assign(price = best_rfr.predict(test_data_mod))\n\nprint(compare_result.head(10))\n\n# Display the performance metric\nprint(f\"Random Forest Best RMSE: {rfr_rmse}\")\nprint(f\"Best Parameters: {grid_rfr.best_params_}\")","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"e0dbf0a5-391e-4611-bccb-5403d1eb0821","outputs":[{"output_type":"stream","name":"stdout","text":"73\n  house_id        city  sale_price  ... bedrooms     house_type   area\n0  1217792  Silvertown       55943  ...        2  Semi-detached  107.8\n1  1900913  Silvertown      384677  ...        5       Detached  498.8\n2  1174927   Riverford      281707  ...        6       Detached  542.5\n3  1773666  Silvertown      373251  ...        6       Detached  528.4\n4  1258487  Silvertown      328885  ...        5       Detached  477.1\n\n[5 rows x 8 columns]\n   bedrooms  avg_price     var_price\n0         2    67076.4  5.652896e+08\n1         3   154665.1  2.378289e+09\n2         4   234704.6  1.725211e+09\n3         5   301515.9  2.484328e+09\n4         6   375741.3  3.924432e+09\nModel RMSE: 21835.347652860648\n     house_id          price\n0     1331375  107393.996770\n1     1630115  300982.868433\n2     1645745  380724.071038\n3     1336775  116321.195539\n4     1888274  266780.641522\n..        ...            ...\n295   1986255  347532.713021\n296   1896276  365763.964228\n297   1758223  254136.806503\n298   1752010  163842.788202\n299   1651404  387786.347359\n\n[300 rows x 2 columns]\n   house_id          price\n0   1331375   79864.142352\n1   1630115  304276.981164\n2   1645745  390803.878225\n3   1336775  102185.087013\n4   1888274  262744.776057\n5   1567679  269626.737607\n6   1298770  340322.114234\n7   1662892   43314.215935\n8   1514321  164050.065218\n9   1795371  205586.910988\nRandom Forest Best RMSE: 14791.408566279288\nBest Parameters: {'max_depth': 8, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 100}\n"}],"execution_count":4}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (User venv)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}